import logging
import os
import re
import time
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup, Tag
from selenium import webdriver
from selenium.common import TimeoutException
from selenium.webdriver.chrome.webdriver import WebDriver
from selenium.webdriver.support.ui import WebDriverWait

from src.base import BaseScraper
from src.config import GOLD_APPLE_PERFUMERY_URL, LOG_DIR
from src.utils import OSUtils
from src.utils import ResponseChecker
from src.validators import PageNumberValidator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
filename = OSUtils.generate_filename('scraping-process', 'log')
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
file_path = os.path.join(LOG_DIR, filename)
file_handler = logging.FileHandler(file_path)
file_formater = logging.Formatter('%(asctime)s ‚Äì %(name)s ‚Äì %(levelname)s: %(message)s')
file_handler.setFormatter(file_formater)
logger.addHandler(file_handler)


class GoldAppleScraper(BaseScraper):
    """ –ö–ª–∞—Å—Å –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ñ—é–º–µ—Ä–∏–∏ —Å —Å–∞–π—Ç–∞ Gold Apple """

    def __init__(self, url: str = GOLD_APPLE_PERFUMERY_URL, page_number: int = 3) -> None:
        """ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–ª–∞—Å—Å–∞ """
        super().__init__(url)
        self._page_number = PageNumberValidator.validate(page_number)

    def _get_webdriver(self) -> WebDriver:
        """ –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ–±-–¥—Ä–∞–π–≤–µ—Ä–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–ø—É—Å–∫–∞ Selenium
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")  # —Ñ–æ–Ω–æ–≤—ã–π —Ä–µ–∂–∏–º –±–µ–∑ –æ–∫–Ω–∞ –±—Ä–∞—É–∑–µ—Ä–∞
        # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ñ–ª–∞–≥–∞ navigator.webdriver = true,
        # –∫–æ—Ç–æ—Ä—ã–π —É–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–π—Ç–∞–º, —á—Ç–æ –±—Ä–∞—É–∑–µ—Ä –∑–∞–ø—É—â–µ–Ω —á–µ—Ä–µ–∑ Selenium
        options.add_argument("--disable-blink-features=AutomationControlled")

        return webdriver.Chrome(options=options)

    def _get_data_structure(self) -> dict:
        """ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–±–∏—Ä–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö """
        return {
            'link': [],
            'name': [],
            'price': [],
            'rating': [],
            'description': [],
            'how_to_use': [],
            'country_of_origin': [],
        }

    @staticmethod
    def _extract_articles(soup: BeautifulSoup) -> list[Tag]:
        """ –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–µ–∫ —Å –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ """
        return soup.find_all('article')

    @staticmethod
    def _extract_product_path(article: Tag) -> str:
        """ –ü–∞—Ä—Å–∏–Ω–≥ URL –ø—É—Ç–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç """
        return article.find('a').get('href')

    @staticmethod
    def _extract_name(soup: BeautifulSoup) -> str:
        """ –ü–∞—Ä—Å–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞ """
        description_div = soup.find('div', {'value': 'Description_0', 'text': re.compile('–û–ø–∏—Å–∞–Ω–∏–µ', re.IGNORECASE)})
        name = description_div.next_element.next_element.get_text(strip=True)
        return name

    @staticmethod
    def _extract_price(soup: BeautifulSoup) -> str:
        """ –ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω—ã –ø—Ä–æ–¥—É–∫—Ç–∞ """
        offers_div = soup.find('div', itemprop='offers')
        price = re.search(r'\d[\d\s]*‚ÇΩ', offers_div.get_text(strip=True)).group() if offers_div else '–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'
        return price

    @staticmethod
    def _extract_rating(soup: BeautifulSoup) -> str:
        """ –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–π—Ç–∏–Ω–≥–∞ –ø—Ä–æ–¥—É–∫—Ç–∞ """
        rating = soup.find('meta', itemprop='ratingValue').get('content')
        return rating

    @staticmethod
    def _extract_description(soup: BeautifulSoup) -> str:
        """ –ü–∞—Ä—Å–∏–Ω–≥ –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞ """
        description = soup.find('div', itemprop='description')
        description = re.sub(r'\n|\s{2,}', ' ', description.get_text(strip=True)) if description else '–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'
        return description

    @staticmethod
    def _extract_how_to_use(soup: BeautifulSoup) -> str:
        """ –ü–∞—Ä—Å–∏–Ω–≥ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –ø—Ä–æ–¥—É–∫—Ç–∞ """
        how_to_use_div = soup.find('div', {'value': 'Text_1', 'text': re.compile('–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ', re.IGNORECASE)})
        how_to_use = re.sub(r'\n', ' ', how_to_use_div.get_text(strip=True)) if how_to_use_div else '–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'
        return how_to_use

    @staticmethod
    def _extract_country_of_origin(soup: BeautifulSoup) -> str:
        """ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω—ã-–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ """
        text_pattern = re.compile('–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', re.IGNORECASE)
        additionally_div = None
        for tab_number in range(2, 6):
            additionally_div = soup.find('div', {'value': f'Text_{tab_number}', 'text': text_pattern})
            if additionally_div:
                break

        if additionally_div:
            additional_info = str(additionally_div.find_next('div'))
            country_of_origin = re.findall(r'(?<=—Å—Ç—Ä–∞–Ω–∞ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è<br/>)([–ê-–Ø–∞-—è]+)(?=<br)', additional_info)
            country_of_origin = country_of_origin[0] if country_of_origin else '–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'
        else:
            country_of_origin = '–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏'
        return country_of_origin

    # def _get_user_agent(self) -> dict:
    #     """ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ User-Agent –¥–ª—è HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤ """
    #     return {'User_Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
    #                            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36')}

    @property
    def page_number(self) -> int:
        """ –ì–µ—Ç—Ç–µ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ """
        return self._page_number

    @page_number.setter
    def page_number(self, value: int) -> None:
        """ –°–µ—Ç—Ç–µ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ """
        self._page_number = PageNumberValidator.validate(value)

    def _scrape_data(self) -> None:
        """ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö """
        logger.info(f'üïµÔ∏è‚Äç‚ôÇÔ∏è –ù–∞—á–∞–ª–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {self._url} '
                    f'| –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {self._page_number}')
        start_scraping_time = time.time()
        ResponseChecker.check(requests.get(self._url))  # –ø—Ä–æ–≤–µ—Ä–∫–∞ HTTP-—Å—Ç–∞—Ç—É—Å–∞ –æ—Ç–≤–µ—Ç–∞
        main_page_html = self._get_main_page_source()
        end_time_getting_html = time.time()
        msg = (f'‚úÖ html —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–ª—É—á–µ–Ω: {self._url} '
               f'| –í—Ä–µ–º—è: {round((end_time_getting_html - start_scraping_time) / 60, 2)} –º–∏–Ω.')
        logger.info(msg)
        print(msg)

        soup = BeautifulSoup(main_page_html, 'lxml')
        articles = self._extract_articles(soup)
        for article in articles:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ URL –ø—Ä–æ–¥—É–∫—Ç–∞
            gold_apple_url = re.search(r'[a-z/:.]+(?=/parfjumerija)', self._url).group()
            product_path = self._extract_product_path(article)
            product_url = urljoin(gold_apple_url, product_path)
            # –ù–∞—á–∞–ª–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞
            st = time.time()  # –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
            product_html = self._get_product_page_source(product_url)
            sp = BeautifulSoup(product_html, 'lxml')
            try:  # –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                name = self._extract_name(sp)
                price = self._extract_price(sp)
                rating = self._extract_rating(sp)
                description = self._extract_description(sp)
                how_to_use = self._extract_how_to_use(sp)
                country_of_origin = self._extract_country_of_origin(sp)
            except AttributeError as exc_info:
                msg = f'‚ùé –û—à–∏–±–∫–∞: {product_url} | –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {exc_info}'
                logger.warning(msg)
                print(msg)
                continue
            else:  # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                self._data['link'].append(product_url)
                self._data['name'].append(name)
                self._data['price'].append(price)
                self._data['rating'].append(rating)
                self._data['description'].append(description)
                self._data['how_to_use'].append(how_to_use)
                self._data['country_of_origin'].append(country_of_origin)
                et = time.time()  # –≤—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞
                pt = round(et - st, 2)  # –≤—Ä–µ–º—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞
                msg = f'‚úÖ –£—Å–ø–µ—à–Ω–æ: {product_url} | –í—Ä–µ–º—è: {pt} —Å–µ–∫.'
                logger.info(msg)
                print(msg)
        end_scraping_time = time.time()
        processing_time = round((end_scraping_time - start_scraping_time) / 60, 2)
        scraped_items = len(self._data['link'])
        logger.info(f'üèÅ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω: {self._url} '
                    f'| –°–ø–∞—Ä—Å–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {scraped_items} | –í—Ä–µ–º—è: {processing_time} –º–∏–Ω.')

    def _get_main_page_source(self) -> str:
        """ –ü–æ–ª—É—á–µ–Ω–∏–µ HTML-–∫–æ–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏ """
        self._webdriver.get(f'{self._url}?p=1')
        self._scrolldown_main_page()
        return self._webdriver.page_source

    def _get_product_page_source(self, product_url) -> str:
        """ –ü–æ–ª—É—á–µ–Ω–∏–µ HTML-–∫–æ–¥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–¥—É–∫—Ç–∞ """
        self._webdriver.get(product_url)
        self._scroll_page_to_bottom()
        return self._webdriver.page_source

    def _scrolldown_main_page(self) -> None:
        """ –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–∞—Ä—Ç–æ—á–∫–∞–º–∏ —Ç–æ–≤–∞—Ä–æ–≤ """
        scrape_pages_list = [str(page) for page in range(1, self._page_number + 1)]
        scroll_step = 300  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∏–∫—Å–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∏ –∑–∞ —Ä–∞–∑
        scroll_pause = 0.7  # –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø—Ä–æ–∫—Ä—É—Ç–∫–æ–π
        # i = 0
        while True:
            self._webdriver.execute_script(f'window.scrollBy(0, {scroll_step});')
            time.sleep(scroll_pause)

            current_url = self._webdriver.current_url
            if not current_url:
                break

            match = re.search(r'(?<=p=)\d+', current_url)
            if not match:
                continue  # –ø—Ä–æ–ø—É—Å–∫ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É URL –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ p

            current_page = match.group()
            # print(f'{i}: {current_page}')
            if current_page not in scrape_pages_list:
                break
            # i += 1

    def _scroll_page_to_bottom(self) -> None:
        """ –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ —Å–∞–º–æ–≥–æ –Ω–∏–∑–∞ —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ """
        wait = WebDriverWait(self._webdriver, 5)  # –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è
        last_height = self._webdriver.execute_script('return document.body.scrollHeight')

        while True:
            self._webdriver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
            try:
                wait.until(lambda driver: driver.execute_script("return document.body.scrollHeight") > last_height)
            except TimeoutException:
                break
            new_height = self._webdriver.execute_script("return document.body.scrollHeight")  # —Å–ª–µ–¥—É—é—â–∞—è –≤—ã—Å–æ—Ç–∞
            if new_height == last_height:
                break

            last_height = new_height
